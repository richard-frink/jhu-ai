{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 8 - Programming Assignment\n",
    "\n",
    "## Directions\n",
    "\n",
    "1. Change the name of this file to be your JHED id as in `jsmith299.ipynb`. Because sure you use your JHED ID (it's made out of your name and not your student id which is just letters and numbers).\n",
    "2. Make sure the notebook you submit is cleanly and fully executed. I do not grade unexecuted notebooks.\n",
    "3. Submit your notebook back in Blackboard where you downloaded this file.\n",
    "\n",
    "*Provide the output **exactly** as requested*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import random\n",
    "import statistics\n",
    "import scipy\n",
    "import math\n",
    "import pprint\n",
    "from typing import List, Dict, Tuple, Callable\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "For this assignment you will be implementing and evaluating a Decision Tree using the ID3 Algorithm (**no** pruning or normalized information gain). Use the provided pseudocode. The data is located at (copy link):\n",
    "\n",
    "http://archive.ics.uci.edu/ml/datasets/Mushroom\n",
    "\n",
    "**Just in case** the UCI repository is down, which happens from time to time, I have included the data and name files on Blackboard.\n",
    "\n",
    "<div style=\"background: lemonchiffon; margin:20px; padding: 20px;\">\n",
    "    <strong>Important</strong>\n",
    "    <p>\n",
    "        No Pandas. The only acceptable libraries in this class are those contained in the `environment.yml`. No OOP, either. You can used Dicts, NamedTuples, etc. as your abstract data type (ADT) for the the tree and nodes.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "One of the things we did not talk about in the lectures was how to deal with missing values. There are two aspects of the problem here. What do we do with missing values in the training data? What do we do with missing values when doing classifcation?\n",
    "\n",
    "For the first problem, C4.5 handled missing values in an interesting way. Suppose we have identifed some attribute *B* with values {b1, b2, b3} as the best current attribute. Furthermore, assume there are 5 observations with B=?, that is, we don't know the attribute value. In C4.5, those 5 observations would be added to *all* of the subsets created by B=b1, B=b2, B=b3 with decreased weights. Note that the observations with missing values are not part of the information gain calculation.\n",
    "\n",
    "This doesn't quite help us if we have missing values when we use the model. What happens if we have missing values during classification? One approach is to prepare for this advance. When you train the tree, you need to add an implicit attribute value \"?\" at every split. For example, if the attribute was \"size\" then the domain would be [\"small\", \"medium\", \"large\", \"?\"]. The \"?\" value gets all the data (because ? is now a wildcard). However, there is an issue with this approach. \"?\" becomes the worst possible attribut value because it has no classification value. What to do? There are several options:\n",
    "\n",
    "1. Never recurse on \"?\" if you do not also recurse on at least one *real* attribute value.\n",
    "2. Limit the depth of the tree.\n",
    "\n",
    "There are good reasons, in general, to limit the depth of a decision tree because they tend to overfit.\n",
    "Otherwise, the algorithm *will* exhaust all the attributes trying to fulfill one of the base cases.\n",
    "\n",
    "You must implement the following functions:\n",
    "\n",
    "`train` takes training_data and returns the Decision Tree as a data structure. There are many options including namedtuples and just plain old nested dictionaries. **No OOP**.\n",
    "\n",
    "```\n",
    "def train(training_data, depth_limit=None):\n",
    "   # returns the Decision Tree.\n",
    "```\n",
    "\n",
    "The `depth_limit` value defaults to None. (What technique would we use to determine the best parameter value for `depth_limit` hint: Module 3!)\n",
    "\n",
    "`classify` takes a tree produced from the function above and applies it to labeled data (like the test set) or unlabeled data (like some new data).\n",
    "\n",
    "```\n",
    "def classify(tree, observations, labeled=True):\n",
    "    # returns a list of classifications\n",
    "```\n",
    "\n",
    "`evaluate` takes a data set with labels (like the training set or test set) and the classification result and calculates the classification error rate:\n",
    "\n",
    "$$error\\_rate=\\frac{errors}{n}$$\n",
    "\n",
    "Do not use anything else as evaluation metric or the submission will be deemed incomplete, ie, an \"F\". (Hint: accuracy rate is not the error rate!).\n",
    "\n",
    "`cross_validate` takes the data and uses 10 fold cross validation (from Module 3!) to `train`, `classify`, and `evaluate`. **Remember to shuffle your data before you create your folds**. I leave the exact signature of `cross_validate` to you but you should write it so that you can use it with *any* `classify` function of the same form (using higher order functions and partial application).\n",
    "\n",
    "Following Module 3's discussion, `cross_validate` should print out the fold number and the evaluation metric (error rate) for each fold and then the average value (and the variance). What you are looking for here is a consistent evaluation metric cross the folds. You should print the error rates in terms of percents (ie, multiply the error rate by 100 and add \"%\" to the end).\n",
    "\n",
    "```\n",
    "def pretty_print_tree(tree):\n",
    "    # pretty prints the tree\n",
    "```\n",
    "\n",
    "This should be a text representation of a decision tree trained on the entire data set (no train/test).\n",
    "\n",
    "To summarize...\n",
    "\n",
    "Apply the Decision Tree algorithm to the Mushroom data set using 10 fold cross validation and the error rate as the evaluation metric. When you are done, apply the Decision Tree algorithm to the entire data set and print out the resulting tree.\n",
    "\n",
    "**Note** Because this assignment has a natural recursive implementation, you should consider using `deepcopy` at the appropriate places.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPIED FROM MODULE 3\n",
    "def parse_data(file_name: str) -> List[List]:\n",
    "    data = []\n",
    "    file = open(file_name, \"r\")\n",
    "    for line in file:\n",
    "        datum = [str(value) for value in line.rstrip().split(\",\")]\n",
    "        data.append(datum)\n",
    "    random.shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p', 'x', 'y', 'y', 'f', 'f', 'f', 'c', 'b', 'g', 'e', 'b', 'k', 'k', 'n', 'p', 'p', 'w', 'o', 'l', 'h', 'y', 'p']\n",
      "8124\n"
     ]
    }
   ],
   "source": [
    "data = parse_data(\"agaricus-lepiota.data\")\n",
    "print(data[0])\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPIED FROM MODULE 3\n",
    "def create_folds(xs: List, n: int) -> List[List[List]]:\n",
    "    k, m = divmod(len(xs), n)\n",
    "    # be careful of generators...\n",
    "    return list(xs[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "folds = create_folds(data, 10)\n",
    "print(len(folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPIED FROM MODULE 3\n",
    "def create_train_test(folds: List[List[List]], index: int) -> Tuple[List[List], List[List]]:\n",
    "    training = []\n",
    "    test = []\n",
    "    for i, fold in enumerate(folds):\n",
    "        if i == index:\n",
    "            test = fold\n",
    "        else:\n",
    "            training = training + fold\n",
    "    return training, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7311\n",
      "813\n"
     ]
    }
   ],
   "source": [
    "training_example, test = create_train_test(folds, 0)\n",
    "print(len(training_example))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Above is directly from Module 3 and 8**\n",
    "\n",
    "## ***Below is new**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluate\"></a>\n",
    "## evaluate\n",
    "\n",
    "This calculates the error rate of the classification.\n",
    "\n",
    "Variables\n",
    "* **data_set** List[List]: list of data_set list\n",
    "* **classification_data** List[List]: list of classification data list\n",
    "\n",
    "**returns** float: the error rate of the classification data from the true data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_set, classification_data):\n",
    "    y = [x[0] for x in data_set]\n",
    "    yh = [x[0] for x in classification_data]\n",
    "    return (sum([1 for i in range(len(y)) if not y[i] == yh[i]]))/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_test1a = ['e','e','e']\n",
    "evaluate_test1b = ['e','e','e']\n",
    "evaluate_test2a = ['e','e','e']\n",
    "evaluate_test2b = ['e','e','p']\n",
    "\n",
    "assert evaluate(evaluate_test1a, evaluate_test1b) == 0\n",
    "assert evaluate(evaluate_test2a, evaluate_test2b) == 1/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"is_homogeneous\"></a>\n",
    "## is_homogeneous\n",
    "\n",
    "This returns a bool for if every label for each record in the data_set is the same.\n",
    "\n",
    "Variables\n",
    "* **data_set** List[List]: list of data_set list\n",
    "\n",
    "**returns** bool: true/false for if the data set is all assigned the same label (assumed to be in index 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_homogeneous(data_set):\n",
    "    labels = [x[0] for x in data_set]\n",
    "    unique_labels = list(set(labels))\n",
    "    return len(unique_labels) < 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ih_test1 = [[1], [1], [1]]\n",
    "ih_test2 = [[1], [2], [1]]\n",
    "\n",
    "assert is_homogeneous(ih_test1) == True\n",
    "assert is_homogeneous(ih_test2) == False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_majority_label\"></a>\n",
    "## get_majority_label\n",
    "\n",
    "This returns the value of the most common label in the dataset.\n",
    "\n",
    "Variables\n",
    "* **data_set** List[List]: list of data_set list\n",
    "\n",
    "**returns** object: will return the most common element, when multiple share highest frequency it will pick the first found as a tie breaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_majority_label(data_set):\n",
    "    labels = [x[0] for x in data_set]\n",
    "    counter = collections.Counter(labels)\n",
    "    return counter.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_test1 = [[1], [1], [1]]\n",
    "gm_test2 = [[1], [2], [2]]\n",
    "gm_test3 = [[3], [2], [1]]\n",
    "gm_test4 = [[1], [2], [3]]\n",
    "\n",
    "assert get_majority_label(gm_test1) == 1\n",
    "assert get_majority_label(gm_test2) == 2\n",
    "assert get_majority_label(gm_test3) == 3\n",
    "assert get_majority_label(gm_test4) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pick_best_attribute\"></a>\n",
    "## pick_best_attribute\n",
    "\n",
    "This goes through the entire data set using the entropy equation to find what element would lower the entropy, then selects that element.\n",
    "\n",
    "Variables\n",
    "* **data_set** List[List]: list of data_set list\n",
    "* **attributes** List: list of indexes that represent the attributes left that can be considered\n",
    "\n",
    "**returns** int: will return the attribute (dataset index) that will cause the entropy to lower the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_best_attribute(data_set, attributes):\n",
    "    lowest_entropy = 9999\n",
    "    entropy_attribute= -1\n",
    "    for attribute in attributes:\n",
    "        domain = list(set([x[attribute] for x in data_set]))\n",
    "        domain_len = len(domain)\n",
    "        entropy = 0\n",
    "        for value in domain:\n",
    "            subset_len = len([x for x in data_set if x[attribute] == value])\n",
    "            entropy -= (subset_len/domain_len)*math.log((subset_len/domain_len), 10)\n",
    "        if entropy < lowest_entropy:\n",
    "            lowest_entropy = entropy\n",
    "            entropy_attribute = attribute\n",
    "    return entropy_attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_domain_for_attribute\"></a>\n",
    "## get_domain_for_attribute\n",
    "\n",
    "Gets the distinct elements that are included in the domain for a given attribute\n",
    "\n",
    "Variables\n",
    "* **data_set** List[List]: list of data_set list\n",
    "* **attribute** int: index/attribute to find values of\n",
    "\n",
    "**returns** List: list of distinct values for the given attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain_for_attribute(data_set, attribute):\n",
    "    return list(set([x[attribute] for x in data_set]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id3\"></a>\n",
    "## id3\n",
    "\n",
    "Recursively create the decision learning tree for a given data_set and attributes predefined for the data_set.\n",
    "\n",
    "Variables\n",
    "* **data_set** List[List]: list of data_set list\n",
    "* **attributes** List: list of indexes that represent the attributes left that can be considered\n",
    "* **default** str: default string value of the label to use in case there is no decision to make\n",
    "\n",
    "**returns** object: this is a collection which represents the entire decision learning tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id3(data_set, attributes, default): \n",
    "    if len(data_set) == 0:\n",
    "        return default\n",
    "    if is_homogeneous(data_set):\n",
    "        return data_set[0][0]\n",
    "    if attributes is None or len(attributes) == 0:\n",
    "        return get_majority_label(data_set)\n",
    "    best_attribute = pick_best_attribute(data_set, attributes)\n",
    "    default_label = get_majority_label(data_set)\n",
    "    attribute_domain = get_domain_for_attribute(data_set, best_attribute)\n",
    "    # are there non-? values in the attribute domain?\n",
    "    recurse_on_questions = len([x for x in attribute_domain if not x == '?']) > 0\n",
    "    children = {}\n",
    "    for value in attribute_domain:\n",
    "        subset = [x for x in data_set if x[best_attribute] == value]\n",
    "        modified_attributes = deepcopy(attributes)\n",
    "        modified_attributes.remove(best_attribute)\n",
    "        if value == '?' and not recurse_on_questions:\n",
    "            children[value] = default_label\n",
    "            continue\n",
    "        child = id3(subset, modified_attributes, default_label)\n",
    "        children[value] = child\n",
    "    return {best_attribute: children}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tree, observations, labeled=True):\n",
    "    current_value = next(iter(tree))\n",
    "    current_tree = tree[current_value]\n",
    "    found = False\n",
    "    while not found:\n",
    "        next_value = observations[current_value]\n",
    "        if not next_value in current_tree:\n",
    "            return \"p\" # the default: we don't want a false positive for being edible or you could die...\n",
    "        current_tree = current_tree[next_value]\n",
    "        if isinstance(current_tree, str):\n",
    "            return current_tree\n",
    "        current_value = next(iter(current_tree))\n",
    "    return \"p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_data, depth_limit=None):\n",
    "    return id3(training_data, [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22], 'p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(evaluation_folds, classify_func, depth_limit=None):\n",
    "    errors = []\n",
    "    for i in range(len(folds)):\n",
    "        train_set, test = create_train_test(evaluation_folds, i)\n",
    "        tree = train(train_set)\n",
    "        test_results = []\n",
    "        for test_row in test:\n",
    "            result = classify(tree, test_row)\n",
    "            data_copy = deepcopy(test_row)\n",
    "            data_copy[0] = result\n",
    "            test_results.append(data_copy)\n",
    "        error = evaluate(test, test_results)\n",
    "        print('\\r\\nFold ' + str(i) + ', Error Rate: ' + str(error*100) + '%')\n",
    "        errors.append(error)\n",
    "    avg_error = sum(errors)/len(errors)\n",
    "    print('\\r\\nAverage error across folds: ' + str(round(avg_error*100, 6)) + '%')\n",
    "    print('Errors Standard Deviation across folds: ' + str(round(statistics.stdev(errors, avg_error), 6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typically the result does not show very well cause the length causes a word wrap, \n",
    "#   it's all there though and if you copy/paste to something like notepad++ it looks good\n",
    "def pretty_print_tree(tree):\n",
    "    pprint.pprint(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0, Error Rate: 51.53751537515375%\n",
      "\n",
      "Fold 1, Error Rate: 52.521525215252154%\n",
      "\n",
      "Fold 2, Error Rate: 49.815498154981555%\n",
      "\n",
      "Fold 3, Error Rate: 54.48954489544895%\n",
      "\n",
      "Fold 4, Error Rate: 52.0935960591133%\n",
      "\n",
      "Fold 5, Error Rate: 54.80295566502463%\n",
      "\n",
      "Fold 6, Error Rate: 52.463054187192114%\n",
      "\n",
      "Fold 7, Error Rate: 50.61576354679803%\n",
      "\n",
      "Fold 8, Error Rate: 50.24630541871922%\n",
      "\n",
      "Fold 9, Error Rate: 49.38423645320197%\n",
      "\n",
      "Average error across folds: 51.796999%\n",
      "Errors Standard Deviation across folds: 0.018546\n",
      "\n",
      "\n",
      "{16: {'p': {6: {'a': {4: {'f': {7: {'c': {8: {'b': {10: {'e': {2: {'s': 'e',\n",
      "                                                                   'y': 'p'}}}}}}}}}},\n",
      "                'f': {17: {'w': {7: {'c': {8: {'b': {4: {'f': {10: {'e': {12: {'k': 'p',\n",
      "                                                                               'y': 'e'}}}},\n",
      "                                                         't': {12: {'f': 'p',\n",
      "                                                                    's': {19: {'e': 'e',\n",
      "                                                                               'p': {18: {'o': {10: {'e': 'e',\n",
      "                                                                                                     't': {11: {'b': {13: {'f': 'p',\n",
      "                                                                                                                           's': {5: {'f': 'p',\n",
      "                                                                                                                                     'n': 'e'}}}}}}}},\n",
      "                                                                                          't': {5: {'n': {10: {'e': {11: {'b': {13: {'s': {14: {'w': {15: {'w': {21: {'v': {20: {'r': 'p',\n",
      "                                                                                                                                                                                 'w': 'e'}},\n",
      "                                                                                                                                                                      'y': 'e'}}}}}}}}}}}}}}}}}}}}}},\n",
      "                                               'n': {18: {'o': {4: {'f': {10: {'e': {14: {'w': {12: {'k': 'p',\n",
      "                                                                                                     's': {15: {'w': {13: {'f': 'e',\n",
      "                                                                                                                           's': {19: {'p': {2: {'f': {5: {'c': 'p',\n",
      "                                                                                                                                                          'n': 'e'}},\n",
      "                                                                                                                                                's': 'p'}}}}}}}}}}}},\n",
      "                                                                               't': 'p'}},\n",
      "                                                                    't': 'p'}}}}}},\n",
      "                                     'w': {14: {'w': {15: {'n': 'e',\n",
      "                                                           'w': {4: {'f': {5: {'c': 'p',\n",
      "                                                                               'n': 'e'}},\n",
      "                                                                     't': {8: {'n': {11: {'b': {12: {'s': {13: {'s': {18: {'o': {19: {'p': {10: {'e': 'p',\n",
      "                                                                                                                                                 't': 'e'}}}}}}}}}}}}}}}}}}}}}},\n",
      "                           'y': 'p'}}}}}}\n"
     ]
    }
   ],
   "source": [
    "cross_validate(folds, classify)\n",
    "print(\"\\r\\n\")\n",
    "result = id3(data, [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22], 'p')\n",
    "pretty_print_tree(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Submit...\n",
    "\n",
    "1. Did you provide output exactly as requested?\n",
    "2. Did you re-execute the entire notebook? (\"Restart Kernel and Rull All Cells...\")\n",
    "3. If you did not complete the assignment or had difficulty please explain what gave you the most difficulty in the Markdown cell below.\n",
    "4. Did you change the name of the file to `jhed_id.ipynb`?\n",
    "\n",
    "Do not submit any other files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (en605645)",
   "language": "python",
   "name": "en605645"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
