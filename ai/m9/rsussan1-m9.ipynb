{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 9 - Programming Assignment\n",
    "\n",
    "## Directions\n",
    "\n",
    "1. Change the name of this file to be your JHED id as in `jsmith299.ipynb`. Because sure you use your JHED ID (it's made out of your name and not your student id which is just letters and numbers).\n",
    "2. Make sure the notebook you submit is cleanly and fully executed. I do not grade unexecuted notebooks.\n",
    "3. Submit your notebook back in Blackboard where you downloaded this file.\n",
    "\n",
    "*Provide the output **exactly** as requested*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "For this assignment you will be implementing and evaluating a Naive Bayes Classifier with the same data from last week:\n",
    "\n",
    "http://archive.ics.uci.edu/ml/datasets/Mushroom\n",
    "\n",
    "(You should have downloaded it).\n",
    "\n",
    "<div style=\"background: lemonchiffon; margin:20px; padding: 20px;\">\n",
    "    <strong>Important</strong>\n",
    "    <p>\n",
    "        No Pandas. The only acceptable libraries in this class are those contained in the `environment.yml`. No OOP, either. You can used Dicts, NamedTuples, etc. as your abstract data type (ADT) for the the tree and nodes.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "You'll first need to calculate all of the necessary probabilities using a `train` function. A flag will control whether or not you use \"+1 Smoothing\" or not. You'll then need to have a `classify` function that takes your probabilities, a List of instances (possibly a list of 1) and returns a List of Tuples. Each Tuple has the best class in the first position and a dict with a key for every possible class label and the associated *normalized* probability. For example, if we have given the `classify` function a list of 2 observations, we would get the following back:\n",
    "\n",
    "```\n",
    "[(\"e\", {\"e\": 0.98, \"p\": 0.02}), (\"p\", {\"e\": 0.34, \"p\": 0.66})]\n",
    "```\n",
    "\n",
    "when calculating the error rate of your classifier, you should pick the class label with the highest probability; you can write a simple function that takes the Dict and returns that class label.\n",
    "\n",
    "As a reminder, the Naive Bayes Classifier generates the *unnormalized* probabilities from the numerator of Bayes Rule:\n",
    "\n",
    "$$P(C|A) \\propto P(A|C)P(C)$$\n",
    "\n",
    "where C is the class and A are the attributes (data). Since the normalizer of Bayes Rule is the *sum* of all possible numerators and you have to calculate them all, the normalizer is just the sum of the probabilities.\n",
    "\n",
    "You will have the same basic functions as the last module's assignment and some of them can be reused or at least repurposed.\n",
    "\n",
    "`train` takes training_data and returns a Naive Bayes Classifier (NBC) as a data structure. There are many options including namedtuples and just plain old nested dictionaries. **No OOP**.\n",
    "\n",
    "```\n",
    "def train(training_data, smoothing=True):\n",
    "   # returns the Decision Tree.\n",
    "```\n",
    "\n",
    "The `smoothing` value defaults to True. You should handle both cases.\n",
    "\n",
    "`classify` takes a NBC produced from the function above and applies it to labeled data (like the test set) or unlabeled data (like some new data). (This is not the same `classify` as the pseudocode which classifies only one instance at a time; it can call it though).\n",
    "\n",
    "```\n",
    "def classify(nbc, observations, labeled=True):\n",
    "    # returns a list of tuples, the argmax and the raw data as per the pseudocode.\n",
    "```\n",
    "\n",
    "`evaluate` takes a data set with labels (like the training set or test set) and the classification result and calculates the classification error rate:\n",
    "\n",
    "$$error\\_rate=\\frac{errors}{n}$$\n",
    "\n",
    "Do not use anything else as evaluation metric or the submission will be deemed incomplete, ie, an \"F\". (Hint: accuracy rate is not the error rate!).\n",
    "\n",
    "`cross_validate` takes the data and uses 10 fold cross validation (from Module 3!) to `train`, `classify`, and `evaluate`. **Remember to shuffle your data before you create your folds**. I leave the exact signature of `cross_validate` to you but you should write it so that you can use it with *any* `classify` function of the same form (using higher order functions and partial application). If you did so last time, you can reuse it for this assignment.\n",
    "\n",
    "Following Module 3's discussion, `cross_validate` should print out the fold number and the evaluation metric (error rate) for each fold and then the average value (and the variance). What you are looking for here is a consistent evaluation metric cross the folds. You should print the error rates in terms of percents (ie, multiply the error rate by 100 and add \"%\" to the end).\n",
    "\n",
    "To summarize...\n",
    "\n",
    "Apply the Naive Bayes Classifier algorithm to the Mushroom data set using 10 fold cross validation and the error rate as the evaluation metric. You will do this *twice*. Once with smoothing=True and once with smoothing=False. You should follow up with a brief explanation for the similarities or differences in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import random\n",
    "import statistics\n",
    "import scipy\n",
    "import math\n",
    "import pprint\n",
    "from typing import List, Dict, Tuple, Callable\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPIED FROM MODULE 3\n",
    "def parse_data(file_name: str) -> List[List]:\n",
    "    data = []\n",
    "    file = open(file_name, \"r\")\n",
    "    for line in file:\n",
    "        datum = [str(value) for value in line.rstrip().split(\",\")]\n",
    "        data.append(datum)\n",
    "    random.shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', 'x', 'y', 'n', 't', 'n', 'f', 'c', 'b', 'p', 't', 'b', 's', 's', 'p', 'p', 'p', 'w', 'o', 'p', 'n', 'y', 'd']\n",
      "8124\n"
     ]
    }
   ],
   "source": [
    "data = parse_data(\"agaricus-lepiota-1.data\")\n",
    "print(data[0])\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPIED FROM MODULE 3\n",
    "def create_folds(xs: List, n: int) -> List[List[List]]:\n",
    "    k, m = divmod(len(xs), n)\n",
    "    # be careful of generators...\n",
    "    return list(xs[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "folds = create_folds(data, 10)\n",
    "print(len(folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPIED FROM MODULE 3\n",
    "def create_train_test(folds: List[List[List]], index: int) -> Tuple[List[List], List[List]]:\n",
    "    training = []\n",
    "    test = []\n",
    "    for i, fold in enumerate(folds):\n",
    "        if i == index:\n",
    "            test = fold\n",
    "        else:\n",
    "            training = training + fold\n",
    "    return training, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7311\n",
      "813\n"
     ]
    }
   ],
   "source": [
    "training_example, test = create_train_test(folds, 0)\n",
    "print(len(training_example))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPIED FROM MODULE 8\n",
    "# checking percentage of records that don't match\n",
    "def evaluate(data_set, classification_data):\n",
    "    y = [x[0] for x in data_set]\n",
    "    yh = [x[0] for x in classification_data]\n",
    "    return (sum([1 for i in range(len(y)) if not y[i] == yh[i]]))/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Above is directly from Modules 3 and 8**\n",
    "\n",
    "## ***Below is new**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"calculate_probabilities\"></a>\n",
    "## calculate_probabilities\n",
    "\n",
    "Recursively create the decision learning tree for a given data_set and attributes predefined for the data_set.\n",
    "\n",
    "Variables\n",
    "* **data_set** List[List]: list of data_set list\n",
    "* **smoothing** bool: this defaults to True and is used to decide whether we should smooth all features as we calculate probabilities\n",
    "\n",
    "**returns** dicitonary of probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probabilities(data_set, smoothing = True):\n",
    "    probabilities = {}\n",
    "    feature_indexes = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22]\n",
    "    smoother = 0\n",
    "    if smoothing:\n",
    "        smoother = 1\n",
    "    label_index = 0\n",
    "    labels = [x[label_index] for x in data_set]\n",
    "    unique_labels = list(set(labels))\n",
    "    for label in unique_labels:\n",
    "        count = len([x for x in labels if x == label])\n",
    "        probabilities[label] = count / len(labels)\n",
    "    for feature_index in feature_indexes:\n",
    "        feature_values = [x[feature_index] for x in data_set]\n",
    "        unique_feature_values = list(set(feature_values))\n",
    "        for feature_value in unique_feature_values:\n",
    "            for label in unique_labels:\n",
    "                working_data = [x for x in data_set if x[0] == label]\n",
    "                count = len([x for x in working_data if x[feature_index] == feature_value])\n",
    "                key = label + str(feature_index) + feature_value\n",
    "                probabilities[key] = (count + smoother) / (len(working_data) + smoother)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i printed out all probabilities and this looks very correct\n",
    "\n",
    "# commenting it out for now though cause it takes up a lot of room\n",
    "#calculate_probabilities(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_probability\"></a>\n",
    "## get_probability\n",
    "\n",
    "Calculates the probability of the data row happening assuming all variables are independent\n",
    "\n",
    "Variables\n",
    "* **row** List: data row representing a mushroom\n",
    "* **label** string: this is the class label we would like to look for\n",
    "* **probabilities** dictionary: this is the dictionary of probabilities\n",
    "\n",
    "**returns** probability as a range of 0.0 through 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability(row, label, probabilities):\n",
    "    probability = probabilities[label]\n",
    "    for x in range(1, len(row)):\n",
    "        key = label + str(x) + row[x]\n",
    "        probability *= probabilities[key]\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probabilities = {\"e\": .5,  \"p\": .5, \"e1a\": .5, \"e1b\": .25, \"e1c\": .25, \"p1b\": .5, \"e2a\": .75, \"p2a\": .1}\n",
    "\n",
    "assert get_probability([\"e,\", \"b\", \"a\"], \"e\", test_probabilities) == 0.09375\n",
    "assert get_probability([\"p\", \"b\", \"a\"], \"p\", test_probabilities) == 0.025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"normalize_results\"></a>\n",
    "## normalize_results\n",
    "\n",
    "Calculates the probability of the data row happening assuming all variables are independent\n",
    "\n",
    "Variables\n",
    "* **results** dictionary of dictionaries: dictionary for the class labels and their respective probabilities for finding them in the data set that generated these results\n",
    "\n",
    "**returns** dictionary of probabilities directly linked to the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_results(results):\n",
    "    p_of_e = results[\"e\"]\n",
    "    p_of_p = results[\"p\"]\n",
    "    total = p_of_e + p_of_p\n",
    "    normalized = {}\n",
    "    normalized[\"e\"] = round(p_of_e / total, 4)\n",
    "    normalized[\"p\"] = round(p_of_p / total, 4)\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_for_normalize1 = {\"e\": .2, \"p\": .6}\n",
    "test_results_for_normalize2 = {\"e\": .1, \"p\": 0.05}\n",
    "\n",
    "assert normalize_results(test_results_for_normalize1) == {'e': 0.25, 'p': 0.75}\n",
    "assert normalize_results(test_results_for_normalize2) == {'e': 0.6667, 'p': 0.3333}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"find_best\"></a>\n",
    "## find_best\n",
    "\n",
    "Finds the class label in the given dictionary that has the highest probability within it.\n",
    "\n",
    "Variables\n",
    "* **results** dictionary of dictionaries: dictionary for the class labels and their respective probabilities for finding them in the data set that generated these results\n",
    "\n",
    "**returns** string (class label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best(results):\n",
    "    max_label = \"\"\n",
    "    max_probability = 0\n",
    "    if results[\"e\"] > max_probability:\n",
    "        max_probability = results[\"e\"]\n",
    "        max_label = \"e\"\n",
    "    if results[\"p\"] > max_probability:\n",
    "        max_probability = results[\"p\"]\n",
    "        max_label = \"p\"\n",
    "    return max_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_for_find_best1 = {'e': 0.25, 'p': 0.75}\n",
    "test_results_for_find_best2 = {'e': 0.9, 'p': 0.1}\n",
    "\n",
    "assert find_best(test_results_for_find_best1) == \"p\"\n",
    "assert find_best(test_results_for_find_best2) == \"e\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"nbc\"></a>\n",
    "## nbc\n",
    "\n",
    "Finds the best class label for the provided row of data.\n",
    "\n",
    "Variables\n",
    "* **probabilities** dictionary: this is the dictionary of probabilities\n",
    "* **row** List: data row representing a mushroom\n",
    "\n",
    "**returns** string class label and the dictionary of all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nbc(probabilities, row):\n",
    "    results = {}\n",
    "    for label in ['e', 'p']:\n",
    "        results[label] = get_probability(row, label, probabilities)\n",
    "    results = normalize_results(results)\n",
    "    best = find_best(results)\n",
    "    return (best, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_data, smoothing=True):\n",
    "    return calculate_probabilities(training_data, smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(probabilities, instances):\n",
    "    results = []\n",
    "    for row in instances:\n",
    "        results.append(nbc(probabilities, row))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(evaluation_folds, smoothing=True):\n",
    "    errors = []\n",
    "    for i in range(len(folds)):\n",
    "        train_set, test = create_train_test(evaluation_folds, i)\n",
    "        probabilities = train(train_set, smoothing)\n",
    "        test_results = []\n",
    "        for test_row in test:\n",
    "            result = classify(probabilities, [test_row])\n",
    "            test_results.append(result[0])\n",
    "        error = evaluate(test, test_results)\n",
    "        e_rounded = round(error*100,6)\n",
    "        print('\\r\\nFold ' + str(i) + ', Error Rate: ' + str(e_rounded) + '%')\n",
    "        errors.append(error)\n",
    "    avg_error = sum(errors)/len(errors)\n",
    "    print('\\r\\nAverage error across folds: ' + str(round(avg_error*100, 6)) + '%')\n",
    "    print('Errors Standard Deviation across folds: ' + str(round(statistics.stdev(errors, avg_error), 6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0, Error Rate: 5.289053%\n",
      "\n",
      "Fold 1, Error Rate: 3.690037%\n",
      "\n",
      "Fold 2, Error Rate: 5.166052%\n",
      "\n",
      "Fold 3, Error Rate: 3.690037%\n",
      "\n",
      "Fold 4, Error Rate: 4.064039%\n",
      "\n",
      "Fold 5, Error Rate: 5.541872%\n",
      "\n",
      "Fold 6, Error Rate: 4.679803%\n",
      "\n",
      "Fold 7, Error Rate: 4.433498%\n",
      "\n",
      "Fold 8, Error Rate: 3.940887%\n",
      "\n",
      "Fold 9, Error Rate: 5.418719%\n",
      "\n",
      "Average error across folds: 4.5914%\n",
      "Errors Standard Deviation across folds: 0.007275\n"
     ]
    }
   ],
   "source": [
    "cross_validate(folds, classify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Submit...\n",
    "\n",
    "1. Did you provide output exactly as requested?\n",
    "2. Did you re-execute the entire notebook? (\"Restart Kernel and Rull All Cells...\")\n",
    "3. If you did not complete the assignment or had difficulty please explain what gave you the most difficulty in the Markdown cell below.\n",
    "4. Did you change the name of the file to `jhed_id.ipynb`?\n",
    "\n",
    "Do not submit any other files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (en605645)",
   "language": "python",
   "name": "en605645"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
